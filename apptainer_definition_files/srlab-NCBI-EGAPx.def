Bootstrap: docker
From: ncbi/egapx:0.5.0

%labels
    Author sam@roberts-lab
    Version 0.5.0
    Description EGAPx - NCBI Eukaryotic Genome Annotation Pipeline configured for HPC/Singularity execution

%help
    This container includes EGAPx 0.5.0 with pre-generated configuration files
    and all dependencies. Designed for HPC systems with Singularity/Apptainer.
    
    Prerequisites:
        - Singularity/Apptainer installed on HPC
        - Sufficient disk space for work directory
        - Internet access for downloading reference data

    Singularity Cache Directory - Set before running (not needed if using the run_egapx.sh script - see below):
    By default, Nextflow will cache Singularity images in the work directory. To specify
    a custom cache location, set the NXF_SINGULARITY_CACHEDIR environment variable:
        
    export NXF_SINGULARITY_CACHEDIR=/path/to/cache

        This is useful for sharing cached images across multiple runs or keeping them in a
        location with sufficient disk space.
    
    Usage:
        singularity run --cleanenv --bind /mmfs1/:/mmfs1/ srlab-NCBI-EGAPx.sif <input.yaml> -e singularity -w <work_dir> -o <output_dir>
    
    Alternatively, use the convenience wrapper script (run_egapx.sh):
        ./run_egapx.sh <input.yaml> -e singularity -w <work_dir> -o <output_dir>
        
        The wrapper script simplifies execution by handling the --cleanenv flag, bind mounts,
        and creating/setting the Singularity cache directory automatically.
        
        Note: The wrapper binds /mmfs1/ by default for HPC filesystem access. Adjust the
        run_egapx.sh script if you need different bind mounts for your system.
    
    
    Example:
        singularity run --cleanenv srlab-NCBI-EGAPx.sif my_genome.yaml -e singularity -w ./work -o ./results
    
    Test run with included example data (~60 minutes):
        singularity run --cleanenv srlab-NCBI-EGAPx.sif /opt/egapx/examples/input_D_farinae_small.yaml \
            -e singularity -w ./work -o ./output
    
    Note: Adjust paths based on your Singularity/Apptainer installation:
        which singularity  # Find binary location
        which apptainer    # Find apptainer location
        # If using older Singularity (not Apptainer), use /etc/singularity instead
    
    The input YAML should specify:
        - genome: path to assembled genome FASTA
        - taxid: NCBI Taxonomy ID
        - short_reads: RNA-seq data (local files or SRA IDs)
    
    See examples in /opt/egapx/examples/ inside the container
    

    
    Advanced - Using Custom Configurations:
    If you need to modify executor settings (memory limits, CPU allocation, etc.),
    you can provide your own egapx_config directory:
    
    1. Extract default configs from container:
        singularity exec srlab-NCBI-EGAPx.sif cp -r /opt/egapx_defaults/egapx_config ./my_custom_config
    
    2. Edit the config files as needed for your environment (e.g., singularity.config for HPC settings)
    
    3. Run with custom configs:
        singularity run --cleanenv --bind ./my_custom_config:./egapx_config \
            srlab-NCBI-EGAPx.sif my_genome.yaml -e singularity -w ./work -o ./results
    
    More info: https://github.com/ncbi/egapx

%post
    # Update package lists
    apt-get update
    
    # Install required packages including singularity
    apt-get install -y git python3-venv wget curl openjdk-17-jre-headless squashfs-tools \
        software-properties-common fuse3 libfuse3-3
    
    # Install singularity from Ubuntu repositories (not used but may be needed for checks)
    apt-get install -y singularity-container
    
    # Install Nextflow v23.10.1
    cd /opt
    wget -qO- https://github.com/nextflow-io/nextflow/releases/download/v23.10.1/nextflow-23.10.1-all | bash
    chmod +x nextflow
    mv nextflow /usr/local/bin/
    
    # Pre-download Nextflow dependencies
    echo "Downloading Nextflow dependencies..."
    nextflow info
    
    # Clone the EGAPx repository
    git clone https://github.com/ncbi/egapx.git
    cd egapx
    
    # Set up Python virtual environment
    python3 -m venv venv
    . venv/bin/activate
    pip install --upgrade pip
    pip install -r requirements.txt
    
    # Generate template config files by running a dry run
    echo "Generating template configuration files..."
    python3 ui/egapx.py examples/input_D_farinae_small.yaml -o /tmp/test_out || true
    
    # Create a permanent location for default configs
    mkdir -p /opt/egapx_defaults
    if [ -d "egapx_config" ]; then
        cp -r egapx_config /opt/egapx_defaults/
        echo "Configuration files generated and stored in /opt/egapx_defaults/egapx_config"
    fi
    
    # Clean up test output
    rm -rf /tmp/test_out
    
    # Create a wrapper script that handles config initialization
    cat > /opt/egapx/egapx_wrapper.sh <<'EOF'
#!/bin/bash
set -e

# Activate Python virtual environment
source /opt/egapx/venv/bin/activate

# If user doesn't have egapx_config in current directory, use defaults
if [ ! -d "$PWD/egapx_config" ]; then
    echo "Using default configuration for execution..."
    if [ -d "/opt/egapx_defaults/egapx_config" ]; then
        cp -r /opt/egapx_defaults/egapx_config "$PWD/"
    else
        echo "Warning: Default configs not found, generating fresh ones..."
        python3 /opt/egapx/ui/egapx.py "$1" -o /tmp/init_out > /dev/null 2>&1 || true
        rm -rf /tmp/init_out
        if [ -d "$PWD/egapx_config" ]; then
            echo "Config files generated successfully"
        fi
    fi
fi

# Configure Singularity options for nested container execution
if [ -f "$PWD/egapx_config/singularity.config" ]; then
    echo "Configuring Singularity for nested container execution..."
    
    # Detect available CPUs from SLURM or system
    CPUS=${SLURM_CPUS_PER_TASK:-$(nproc)}
    echo "Detected $CPUS CPUs available"
    
    # Add bind mounts and environment variables to singularity.config
    cat >> "$PWD/egapx_config/singularity.config" <<SINGCONFIG

// Additional configuration for nested Singularity execution
// Disable container usage - processes run directly since we're already in EGAPx container
docker.enabled = false
singularity.enabled = false

process {
    cpus = { $CPUS }
    maxForks = $CPUS
}
SINGCONFIG
fi

# Run EGAPx with all arguments from the user's working directory
exec python3 /opt/egapx/ui/egapx.py "$@"
EOF
    
    chmod +x /opt/egapx/egapx_wrapper.sh
    
    # Clean up
    apt-get clean
    rm -rf /var/lib/apt/lists/*

%environment
    export PATH=/opt/egapx:$PATH
    export LC_ALL=C

%runscript
    exec /opt/egapx/egapx_wrapper.sh "$@"

%test
    # Verify the installation
    cd /opt/egapx
    source venv/bin/activate
    python3 ui/egapx.py --help
    echo "EGAPx container test passed!"
