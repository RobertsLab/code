---
title: "20240202-e5-coral-sRNAseq-merging-explorations"
author: "Sam White"
date: "2024-02-02"
output: 
  github_document:
    toc: true
    number_sections: true
  bookdown::html_document2:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
---

# INTRO

Our previous sRNA-seq data analysis for this project revealed that the R2 reads didn't seem to match with anything (specifically, no clade assignments when using [miRTrace](https://github.com/friedlanderlab/mirtrace)), which seemed odd. Since the insert sizes should be small for these libraries and the read lengths kind of long (150bp PE), we'd expect the resulting reads to overlap. And, in turn, R2 reads should be reverse complements of R1 reads (and vice versa). After discussing things with Azenta (company which performed library construction and sequencing), they indicated that their in-house pipeline discards the R2 reads because they come from the opposite strand and most miRNAs are found on the R1 strand.

It seemed like a bit of waste to discard all the data that exists in the R2 reads, so I wanted to explore merging the reads to see if we could preserve the R2 data. Additionally, I came across [XICRA](https://github.com/HCGB-IGTP/XICRA) (GitHub repo) [@sanchezherrero2021] which describes using paired-end sequence data to identify miRNA isoforms (isomiRs), suggesting that we should try to retain the R2 reads to improve our miRNA analyses.

I decided to explore merging/trimming using two different sets of software:

-   [BBMerge](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbmerge-guide/) [@bushnell2017]

-   [`fastp`](https://github.com/OpenGene/fastp)[@chen2023; @chen2018]

[BBMerge](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbmerge-guide/) seemed like a good tool, as it's specifically designed to handle merging of Illumina PE sequencing reads. Plus, it's part of the [BBTols suite](https://sourceforge.net/projects/bbmap/), which has a number of other tools, and packages like this usually have nice cross-compatibility between the different tools.

[`fastp`](https://github.com/OpenGene/fastp) has both trimming *and* merging built into the tools itself, which I hadn't previously realized (but, had never explored, since this isn't how we usually work with PE sequencing data). I was simply going to use [`fastp`](https://github.com/OpenGene/fastp) for trimming, as it's the trimmer thta I'm most familiar with. I honestly thought I'd trim with [`fastp`](https://github.com/OpenGene/fastp), and then merge with [BBMerge](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbmerge-guide/), but the fact that [`fastp`](https://github.com/OpenGene/fastp) had it built-in could make my life easier.

So, with all of that out of the way, let's start the analysis. This will only use a single set of PE sequencing reads, as an initial exploration. We'll decide on the best approach and then use that approach to run all the sequencing data through the chosen "pipeline."

# SETUP

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  echo = TRUE,         # Display code chunks
  eval = FALSE,        # Evaluate code chunks
  warning = FALSE,     # Hide warnings
  message = FALSE,     # Hide messages
  comment = "",        # Prevents appending '##' to beginning of lines in code output
  width = 1000         # adds scroll bar
)
```

## Create a Bash variables file

This allows usage of Bash variables across R Markdown chunks.

```{r save-bash-variables-to-rvars-file, engine='bash', eval=TRUE}
{
echo "#### Assign Variables ####"
echo ""

echo "## Data URL"
echo 'export raw_reads_url="https://owl.fish.washington.edu/nightingales/P_evermanni/30-852430235/"'

echo "# Data directories"
echo 'export fastqc_out_dir="./fastqc"'
echo 'export raw_reads_dir="./raw"'
echo 'export trimmed_fastqs_dir="./trimmed-reads"'
echo ""

echo "## NEB nebnext-small-rna-library-prep-set-for-illumina adapters"
echo 'export first_adapter="AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC"'
echo 'export second_adapter="GATCGTCGGACTGTAGAACTCTGAACGTGTAGATCTCGGTGGTCGCCGTATCATT"'
echo ""

echo "# Input/output files"
echo 'export NEB_adapters_fasta=NEB-adapters.fasta'
echo ""

echo "# Paths to programs"
echo 'export programs_dir="/home/shared"'
echo 'export bbmerge="${programs_dir}/bbmap-39.06/bbmerge.sh"'
echo 'export fastp="${programs_dir}/fastp"'
echo 'export fastqc="${programs_dir}/FastQC-0.12.1/fastqc"'
echo 'export multiqc=/home/sam/programs/mambaforge/bin/multiqc'
echo ""

echo "# Set number of CPUs to use"
echo 'export threads=46'
echo ""

echo "# Initialize arrays"
echo 'export trimmed_fastqs_array=()'


} > .bashvars

cat .bashvars
```

## Download raw sRNAseq reads

Reads are downloaded from <https://owl.fish.washington.edu/nightingales/P_evermanni/30-852430235/>

The `--cut-dirs 3` command cuts the preceding directory structure (i.e. `nightingales/P_evermanni/30-852430235/`) so that we just end up with the reads.

```{bash download-raw-reads, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

mkdir --parents "${raw_reads_dir}" "${trimmed_fastqs_dir}"

wget \
--directory-prefix ${raw_reads_dir} \
--recursive \
--no-check-certificate \
--continue \
--cut-dirs 3 \
--no-host-directories \
--no-parent \
--quiet \
--accept "sRNA-POR-79*,checksums.md5" ${raw_reads_url}

ls -lh "${raw_reads_dir}"
```

### Verify raw read checksums

```{bash verify-raw-read-checksums, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

cd "${raw_reads_dir}"

# Checksums file contains other files, so this just looks for the sRNAseq files.
grep "sRNA-POR-79" checksums.md5 | md5sum --check
```

## Create adapters FastA

```{bash create-FastA-of-adapters, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

echo "Creating adapters FastA."
echo ""
adapter_count=0

# Check for adapters file first
# Then create adapters file if doesn't exist
if [ -f "${NEB_adapters_fasta}" ]; then
  echo "${NEB_adapters_fasta} already exists. Nothing to do."
else
  for adapter in "${first_adapter}" "${second_adapter}"
  do
    adapter_count=$((adapter_count + 1))
    printf ">%s\n%s\n" "adapter_${adapter_count}" "${adapter}"
  done >> "${NEB_adapters_fasta}"
fi

echo ""
echo "Adapters FastA:"
echo ""
cat "${NEB_adapters_fasta}"
echo ""
```

## [`fastp`](https://github.com/OpenGene/fastp)

[`fastp`](https://github.com/OpenGene/fastp) has many built-in functions, including before/after trimming plots, and read 1 and read 2 merging.

::: callout-note
Setting `--overlap_len_require 17` forces [`fastp`](https://github.com/OpenGene/fastp) to look at reads starting at base 17, instead of the default 30 and results in more accurate analyses, since these are sRNA-seq libraries where the expected/desired read lengths will be 17 - 30bp in length.
:::

### Paired-end Trimming

The paired end trimming will examine how some of the trimming options impact sequencing read lengths, quality, and subsequent merging.

#### PolyG Trimming Only

```{bash fastp-polyG-only, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

"${fastp}" \
--in1 ./raw/sRNA-POR-79-S1-TP2_R1_001.fastq.gz \
--in2 ./raw/sRNA-POR-79-S1-TP2_R2_001.fastq.gz \
--out1 ./trimmed-reads/fastp-polyG_only.R1.fq.gz \
--out2 ./trimmed-reads/fastp-polyG_only.R2.fq.gz \
--disable_adapter_trimming \
--trim_poly_g \
--overlap_len_require 17 \
--thread ${threads} \
--html "fastp-polyG-only.html" \
--json "fastp-polyG-only.json" \
--report_title "fastp-polyG-only"

```

#### Adapters & PolyG Trimming

```{bash fastp-adapters-and-polyg, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

"${fastp}" \
--in1 ./raw/sRNA-POR-79-S1-TP2_R1_001.fastq.gz \
--in2 ./raw/sRNA-POR-79-S1-TP2_R2_001.fastq.gz \
--out1 ./trimmed-reads/fastp-adapters-polyG.R1.fq.gz \
--out2 ./trimmed-reads/fastp-adapters-polyG.R2.fq.gz \
--adapter_fasta NEB-adapters.fasta \
--trim_poly_g \
--overlap_len_require 17 \
--thread ${threads} \
--html "fastp-adapters-polyG.html" \
--json "fastp-adapters-polyG.json" \
--report_title "fastp-adapters-polyG-only"
```

#### PolyG Trimming Max Length 31

This can't be run because we know from the polyG only trimming, that most read lengths are \>89bp. As such, it will end up discarding all the reads...

#### Adapters & PolyG Trimming Max Length 31

::: callout-note
The max length is based on the `fastp` insert peak size from the adapter and polyG trimming results, and previous evaluation of mean read lengths via [`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and [`MultiQC`](https://multiqc.info/).
:::

```{bash fastp-adapters-polyG-31bp, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

"${fastp}" \
--in1 ./raw/sRNA-POR-79-S1-TP2_R1_001.fastq.gz \
--in2 ./raw/sRNA-POR-79-S1-TP2_R2_001.fastq.gz \
--out1 ./trimmed-reads/fastp-adapters-polyG-31bp.R1.fq.gz \
--out2 ./trimmed-reads/fastp-adapters-polyG-31bp.R2.fq.gz \
--adapter_fasta NEB-adapters.fasta \
--trim_poly_g \
--overlap_len_require 17 \
--length_limit 31 \
--thread ${threads} \
--html "fastp-adapters-polyG-31bp.html" \
--json "fastp-adapters-polyG-31bp.json" \
--report_title "fastp-adapters-polyG-31bp"
```

#### Adapters & PolyG Trimming Max Length 31 With Merge

This will attempt to merge the R1 and R2 reads.

```{bash fastp-adapters-polyG-31bp-merged, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

"${fastp}" \
--in1 ./raw/sRNA-POR-79-S1-TP2_R1_001.fastq.gz \
--in2 ./raw/sRNA-POR-79-S1-TP2_R2_001.fastq.gz \
--adapter_fasta NEB-adapters.fasta \
--trim_poly_g \
--overlap_len_require 17 \
--length_limit 31 \
--merge \
--merged_out ./trimmed-reads/fastp-adapters-polyG-31bp-merged.fq.gz \
--thread ${threads} \
--html "fastp-adapters-polyG-31bp-merged.html" \
--json "fastp-adapters-polyG-31bp-merged.json" \
--report_title "fastp-adapters-polyG-31bp-merged"
```

### Single-end Trimming - R1 Reads Only

Using just the R1 reads, per Azenta's recommendation.

#### Auto-adapters & PolyG Trimming

```{bash fastp-R1-auto_adapters-and-polyg, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

"${fastp}" \
--in1 ./raw/sRNA-POR-79-S1-TP2_R1_001.fastq.gz \
--out1 ./trimmed-reads/fastp-R1-auto_adapters-polyG.fq.gz \
--trim_poly_g \
--thread ${threads} \
--html "fastp-R1-auto_adapters-polyG.html" \
--json "fastp-R1-auto_adapters-polyG.json" \
--report_title "fastp-R1-auto_adapters-polyG"
```

#### Auto-adapters & PolyG Trimming Max Length 31

::: callout-note
The max length is based on the `fastp` insert peak size from the adapter and polyG trimming results, and previous evaluation of mean read lengths via [`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and [`MultiQC`](https://multiqc.info/).
:::

```{bash fastp-R1-auto_adapters-polyG-31bp, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

"${fastp}" \
--in1 ./raw/sRNA-POR-79-S1-TP2_R1_001.fastq.gz \
--out1 ./trimmed-reads/fastp-R1-31bp-auto_adapters-polyG.fq.gz \
--trim_poly_g \
--length_limit 31 \
--thread ${threads} \
--html "fastp-R1-auto_adapters-polyG-31bp.html" \
--json "fastp-R1-auto_adapters-polyG-31bp.json" \
--report_title "fastp-R1-auto_adapters-polyG-31bp"
```

## BBMerge

### Raw reads

[BBMerge](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbmerge-guide/) documentations actually indicates that merging performs best using *raw*, untrimmed reads, so we'll try that out first...

```{r bbmerge-raw-reads, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars
${bbmerge} \
in1=./raw/sRNA-POR-79-S1-TP2_R1_001.fastq.gz \
in2=./raw/sRNA-POR-79-S1-TP2_R2_001.fastq.gz \
adapters=./NEB-adapters.fasta \
mininsert=17 \
out=./trimmed-reads/sRNA-POR-79-S1-TP2-raw-bbmerge.fq.gz
```

### Trimmed PolyG Only

Running [BBMerge](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbmerge-guide/) on just raw reads yielded poor results (\>99% of reads could *not* be merged). Additionally, the average insert size was determined to be 131.5bp, which is not going to be useful for sRNA-seq.

I know that there's a very large stretch of polyG sequence in the reads (likely due to the small insert size and "long" read length - Gs get added as a "placeholder" base each cycle that exceeds the insert size), so let's try merging reads *after* polyG sequence has been trimmed.

```{r bbmerge-trimmed-polyg-only, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

${bbmerge} \
in1=./trimmed-reads/fastp-polyG_only.R1.fq.gz \
in2=./trimmed-reads/fastp-polyG_only.R2.fq.gz \
adapters=./NEB-adapters.fasta \
mininsert=17 \
out=./trimmed-reads/sRNA-POR-79-S1-TP2-fastp-polyG_only-bbmerge.fq.gz
```

The results from using polyG-trimmed reads are certainly much better than just merging raw reads (12,649,853 reads could be joined this time). Mean insert size is now determined to be 30bp, which is what we might expect. However, the insert range (17 - 289) is still concerning, as reads \>30bp likely aren't usable. Adapter sequences are still present, which are certainly contributing to some of the inability to merge, as well as the large read lengths. So, let's try merging reads that have been trimmed of both polyG and adapter sequences.

### Trimmed Adapters & PolyG

```{r bbmerge-trimmed-adapters-and-polyG, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

${bbmerge} \
in1=./trimmed-reads/fastp-adapters-polyG.R1.fq.gz \
in2=./trimmed-reads/fastp-adapters-polyG.R2.fq.gz \
adapters=./NEB-adapters.fasta \
mininsert=17 \
out=./trimmed-reads/sRNA-POR-79-S1-TP2-fastp-adapters-polyG-bbmerge.fq.gz
```

Well, trimming adapters certainly made a difference, as well! Now, 15,533,284 reads get joined (as opposed to 12,649,853 when using just polyG-trimmed reads). The average insert size changes very little, but, oddly, the insert range remains the same (17 - 289).

For kicks, let's throw the reads that have been trimmed to 31bp into [BBMerge](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbmerge-guide/) and see what happens.

### Trimmed Adapters & PolyG Max Length 31bp

```{r bbmerge-trimmed-adapters-polyG-31bp, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

${bbmerge} \
in1=./trimmed-reads/fastp-adapters-polyG-31bp.R1.fq.gz \
in2=./trimmed-reads/fastp-adapters-polyG-31bp.R2.fq.gz \
adapters=./NEB-adapters.fasta \
mininsert=17 \
out=./trimmed-reads/sRNA-POR-79-S1-TP2-fastp-adapters-polyG-31bp-bbmerge.fq.gz
```

This resulted in 12,346,728 reads being joined. This is on par with the 12,649,853 reads joined when using just polyG-trimmed reads. Additionally, we see the average insert size is 26.3bp, which is even more inline with what we'd expect for miRNAs (22-24bp), and the insert range is also inline with what we'd expect for these libraries (17 - 35bp). However, this is about 20% fewer reads than the 15,533,284 reads when using polyG- and adapter-trimmed reads.

Maybe I'll test out the subsequent downstream analyses using these different merging results...

## FastQC

```{r fastqc, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

# Populate array with FastQ files
fastq_array=(./trimmed-reads/*.fq.gz)

# Pass array contents to new variable
fastqc_list=$(echo "${fastq_array[*]}")

# Run FastQC
# NOTE: Do NOT quote ${fastqc_list}
${fastqc} \
--threads ${threads} \
--quiet \
--outdir ./ \
${fastqc_list}
```

## MultiQC

```{r multiqc, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

${multiqc} .
```

Check out the [`MultiQC`](https://multiqc.info/) results here (HTML):

-   [MultiQC Report](./multiqc_report.html)
